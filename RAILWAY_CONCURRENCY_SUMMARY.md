# UniLingo Railway Backend - Concurrency Summary

**Quick Reference Guide** | October 12, 2025

---

## Current Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Clients   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTPS
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Railway Web Service    â”‚
â”‚  (Single Instance)      â”‚
â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Express.js       â”‚  â”‚
â”‚  â”‚  â€¢ Async/Await    â”‚  â”‚
â”‚  â”‚  â€¢ Rate Limiting  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  In-Memory Queues â”‚  â”‚
â”‚  â”‚  â€¢ OpenAI (1x)    â”‚  â”‚
â”‚  â”‚  â€¢ Azure (20x)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Inline API Calls
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   External APIs          â”‚
â”‚  â€¢ OpenAI (5-30s)        â”‚
â”‚  â€¢ Azure Speech (2-10s)  â”‚
â”‚  â€¢ Azure OCR (1-3s)      â”‚
â”‚  â€¢ AWS Polly (2-15s)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**âŒ No Workers | âŒ No Redis | âŒ No Auto-scaling**

---

## Key Findings

### âœ… What's Working
- **Async I/O**: Node.js handles multiple connections efficiently
- **Rate Limiting**: Per-IP and per-user limits in place
- **Circuit Breakers**: Fail-safe for API failures
- **Monitoring**: Comprehensive metrics and health checks
- **Request Queuing**: In-memory queues for OpenAI and Azure

### ğŸš¨ Critical Issues

| Issue | Impact | Risk Level |
|-------|--------|------------|
| **No horizontal scaling** | Single point of failure | ğŸ”´ High |
| **Blocking API calls** | 5-30s HTTP response times | ğŸ”´ High |
| **In-memory queues** | Lost on restart | ğŸ”´ High |
| **Sequential OpenAI** | 1 request at a time | ğŸŸ¡ Medium |
| **No background workers** | Timeouts under load | ğŸ”´ High |

---

## How Concurrent Requests Are Handled

### Example: 10 Users Generate Flashcards Simultaneously

**Current Behavior:**
```
User 1:  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 20s â†’ Response
User 2:     [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 19s â†’ Response
User 3:        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 12s â†’ Response
User 4:           [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 7s â†’ Response
User 5:              [â–ˆâ–ˆ] 2s â†’ Response
...
Total: 79 seconds for 10 users
```

**Problem:** Each request blocks HTTP response while waiting for OpenAI

**What Should Happen (with workers):**
```
All Users: [âœ“] 0.2s â†’ Job ID returned
           [Background processing: 20s]
           [Webhook notification]
           
Total: 0.2 seconds for all users
```

---

## API Concurrency Patterns

| Service | Pattern | Concurrency | Blocking Time |
|---------|---------|-------------|---------------|
| **OpenAI** | Sequential queue | 1 at a time | 5-30 seconds |
| **Azure Speech** | Parallel queue | Max 20 | 2-10 seconds |
| **Azure OCR** | Direct call | No limit | 1-3 seconds |
| **AWS Polly** | Direct call | No limit | 2-15 seconds |
| **PDF Parse** | Direct call | No limit | 5-20 seconds |

### Rate Limits

```
General (IP):        100 requests / 15 minutes
Pronunciation (IP):  10 requests / minute
Pronunciation (User): 100 requests / hour
AI (IP):             20 requests / minute
AI (User):           200 requests / hour

OpenAI (App):        50 requests / minute
OpenAI (App):        75,000 tokens / minute
Azure Speech:        20 concurrent connections
```

---

## Bottleneck Analysis

### 1. OpenAI Sequential Processing ğŸ”´
```javascript
// backend/aiService.js (Lines 99-145)
while (requestQueue.length > 0) {
  const request = requestQueue.shift();
  const result = await request.execute(); // â³ Blocks here
  request.resolve(result);
}
```
**Impact:** Second user waits for first user to complete  
**Solution:** Process 5 requests concurrently instead of 1

### 2. In-Memory Queues ğŸ”´
```javascript
let requestQueue = []; // âŒ Lost on restart
```
**Impact:** Requests lost on deployment or crash  
**Solution:** Use Redis + BullMQ for persistent queues

### 3. No Background Workers ğŸ”´
```javascript
app.post('/api/ai/generate-flashcards', async (req, res) => {
  const result = await AIService.generateFlashcards(...); // â³ Blocks HTTP
  res.json(result);
});
```
**Impact:** HTTP timeout risk (Railway default: 30s)  
**Solution:** Return job ID immediately, process in background

---

## Recommended Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Clients   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web Service            â”‚ â—„â”€â”
â”‚  (Auto-scale 1-5)       â”‚   â”‚ Read status
â”‚  â€¢ Return job IDs       â”‚   â”‚
â”‚  â€¢ No blocking (< 200ms)â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
          â”‚                   â”‚
          â–¼                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  Redis                  â”‚   â”‚
â”‚  â€¢ Job queue (BullMQ)   â”‚â”€â”€â”€â”˜
â”‚  â€¢ Rate limits (shared) â”‚
â”‚  â€¢ Circuit breaker      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker Service         â”‚
â”‚  (Auto-scale 1-3)       â”‚
â”‚  â€¢ OpenAI (5x parallel) â”‚
â”‚  â€¢ Audio generation     â”‚
â”‚  â€¢ Long operations      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    External APIs
```

---

## Implementation Plan

### Week 1: Foundation
```bash
# Add Redis
railway plugin:add redis

# Install dependencies
npm install bullmq ioredis

# Create worker
touch backend/worker.js
```

**Changes:**
- Move OpenAI flashcard generation to background
- Return job IDs instead of results
- Test job queue functionality

### Week 2: Migration
- Move lesson generation to worker
- Move audio generation to worker
- Add job status polling endpoint
- Update frontend to handle async jobs

### Week 3: Scaling
- Enable Railway auto-scaling (min: 1, max: 5)
- Move rate limits to Redis
- Add Sentry APM
- Configure alerts

### Week 4: Optimization
- Parallel OpenAI processing (5 concurrent)
- Response caching with Redis
- Load testing
- Documentation

---

## Cost Impact

| Configuration | Monthly Cost | Capacity |
|--------------|--------------|----------|
| **Current** | $40-70 | ~100 active users, 20 req/min |
| **Recommended** | $70-140 | ~1,000 active users, 200 req/min |
| **ROI** | +100% cost | +1,000% capacity |

---

## Performance Comparison

| Metric | Current | With Workers |
|--------|---------|--------------|
| HTTP response time | 5-30 seconds | < 200ms |
| Max throughput | ~20 AI req/min | ~200 AI req/min |
| Queue persistence | âŒ Lost on restart | âœ… Redis-backed |
| Scaling | âŒ Single instance | âœ… Auto-scale |
| Concurrent OpenAI | 1 request | 5 requests |

---

## Critical Next Steps

1. **Add Redis** to Railway project
2. **Create worker service** (`backend/worker.js`)
3. **Migrate flashcard generation** to background job
4. **Test with staging environment**
5. **Deploy to production** with monitoring

---

## Quick Test

To see the concurrency issue in action:

```bash
# Send 5 flashcard requests simultaneously
for i in {1..5}; do
  curl -X POST https://your-railway-app.com/api/ai/generate-flashcards \
    -H "Content-Type: application/json" \
    -d '{"content":"test","subject":"test","topic":"test","userId":"test"}' &
done

# Watch logs - you'll see sequential processing
```

**Current behavior:** 5 requests take ~60 seconds (12s each)  
**Expected with workers:** All return job IDs in < 1 second

---

## Questions?

**Contact:** Development Team  
**Full Report:** `RAILWAY_BACKEND_ANALYSIS.md`  
**Diagram:** `RAILWAY_ARCHITECTURE_DIAGRAM.txt`

---

**Report Date:** October 12, 2025  
**Version:** 1.0

